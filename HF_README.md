---
title: Neural Pong - Diffusion Model Game
emoji: üéÆ
colorFrom: blue
colorTo: purple
sdk: gradio
sdk_version: 4.0.0
app_file: app.py
pinned: false
license: mit
---

# üéÆ Neural Pong - A Diffusion Model Powered Game

An interactive Pong game where **every single frame is generated by a diffusion model** in real-time! This is a demonstration of using causal diffusion transformers with KV-caching for real-time video generation.

## üéØ What makes this special?

Unlike traditional games that use pre-programmed physics and rendering, this Pong game:
- **Generates each frame** using a diffusion model
- **Learns game dynamics** from 1M training frames
- **Responds to your actions** through action-conditioned generation
- **Runs in real-time** (~4 FPS) thanks to KV-caching

## üéÆ How to Play

1. Click **"Start Game"** to begin
2. Use **Arrow Keys (‚Üë/‚Üì)** or **W/S** keys to control your paddle
3. Watch as the model generates each frame based on your actions!

## üèóÔ∏è Architecture

**Model:** Diffusion Transformer (DiT)
- 8 transformer layers
- 12 attention heads
- 384 hidden dimensions
- Causal KV-caching for efficient sequential generation

**Training:**
- Dataset: 1M Pong game frames
- Objective: Diffusion forcing
- Action conditioning: Discrete actions (START, NOOP, UP, DOWN)

**Inference:**
- 4 diffusion steps per frame
- bfloat16 mixed precision
- Real-time generation with KV-cache

## üì¶ Setup for Hugging Face Spaces

### Step 1: Create a new Space

1. Go to [huggingface.co/spaces](https://huggingface.co/spaces)
2. Click "Create new Space"
3. Choose:
   - Space name: `neural-pong` (or your choice)
   - License: MIT
   - Space SDK: **Gradio**
   - Space hardware: **GPU** (required!)

### Step 2: Upload your files

Clone your new Space repository:

```bash
git clone https://huggingface.co/spaces/YOUR_USERNAME/neural-pong
cd neural-pong
```

Copy the necessary files from this repository:

```bash
# Copy application files
cp /path/to/toy-wm/app.py .
cp /path/to/toy-wm/requirements.txt .
cp /path/to/toy-wm/HF_README.md README.md

# Copy source code
cp -r /path/to/toy-wm/src .
cp -r /path/to/toy-wm/configs .

# Copy datasets (optional, only if needed)
# cp -r /path/to/toy-wm/datasets .
```

### Step 3: Upload model checkpoint

You have two options for the model checkpoint:

#### Option A: Upload to Hugging Face Hub (Recommended)

```python
from huggingface_hub import HfApi, create_repo

# Create a model repository
repo_id = "YOUR_USERNAME/neural-pong-model"
create_repo(repo_id, repo_type="model", exist_ok=True)

# Upload checkpoint
api = HfApi()
api.upload_file(
    path_or_fileobj="experiments/radiant-forest-398/ckpt-step=053700-metric=0.00092727.pt",
    path_in_repo="model.pt",
    repo_id=repo_id,
    repo_type="model"
)
```

Then update `configs/inference.yaml` to use the HF Hub path:

```yaml
model:
  checkpoint: "hf://YOUR_USERNAME/neural-pong-model/model.pt"
```

#### Option B: Include in Space with Git LFS

```bash
# Initialize Git LFS
git lfs install

# Track large files
git lfs track "experiments/**/*.pt"
git lfs track "*.pt"

# Copy checkpoint
mkdir -p experiments/radiant-forest-398
cp /path/to/checkpoint.pt experiments/radiant-forest-398/ckpt-step=053700-metric=0.00092727.pt

# Add .gitattributes
git add .gitattributes
```

### Step 4: Commit and push

```bash
git add .
git commit -m "Initial commit: Neural Pong game"
git push
```

### Step 5: Configure Space settings

1. Go to your Space settings on Hugging Face
2. Under "Space hardware", select **T4 small** or better
3. Set the Space to **Public** (or Private if preferred)
4. Enable **Always On** if you want 24/7 availability (optional)

## üöÄ Local Development

To run locally:

```bash
# Clone the repository
git clone https://huggingface.co/spaces/YOUR_USERNAME/neural-pong
cd neural-pong

# Install dependencies
pip install -r requirements.txt

# Run the app
python app.py
```

The app will be available at `http://localhost:7860`

## üìù Notes

- **GPU Required:** This model requires a CUDA-capable GPU to run
- **Loading Time:** First startup takes 30-60 seconds to load the model
- **Performance:** Expect ~4 FPS on T4 GPU, higher on more powerful GPUs
- **Browser Compatibility:** Works best in Chrome/Firefox with keyboard support

## üîß Troubleshooting

### "CUDA not available" error
- Make sure your Space is configured to use GPU hardware
- Check that PyTorch with CUDA support is installed

### Slow loading
- Model loading can take time on first run
- Consider using Option A (HF Hub) for faster checkpoint loading

### Black screen
- Wait for the model to fully load (check console logs)
- Try refreshing the page
- Ensure keyboard focus is on the page before pressing keys

## üìö Learn More

This project demonstrates:
- Real-time diffusion model inference
- KV-caching for efficient transformers
- Action-conditioned video generation
- Gradio for interactive ML demos

## üôè Credits

Built with:
- PyTorch & Diffusion Models
- Gradio for the web interface
- Hugging Face Spaces for hosting

## üìÑ License

MIT License - feel free to use and modify!

---

**Tip:** For best experience, ensure the page has focus and use keyboard controls!




